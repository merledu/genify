{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-2.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.3 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: numpy>=1.22.4 in ./.venv/lib/python3.10/site-packages (from pandas) (2.2.6)\n",
      "Collecting tzdata>=2022.7\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m347.8/347.8 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting pytz>=2020.1\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Installing collected packages: pytz, tzdata, pandas\n",
      "Successfully installed pandas-2.3.1 pytz-2025.2 tzdata-2025.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: scikit-learn in ./.venv/lib/python3.10/site-packages (1.7.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./.venv/lib/python3.10/site-packages (from scikit-learn) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./.venv/lib/python3.10/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: scipy>=1.8.0 in ./.venv/lib/python3.10/site-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: numpy>=1.22.0 in ./.venv/lib/python3.10/site-packages (from scikit-learn) (2.2.6)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Absolute Maximum Scaling\n",
    "\n",
    "This method of scaling requires two-step:\n",
    "\n",
    "1. We should first select the maximum absolute value out of all the entries of a particular measure.\n",
    "2. Then after this we divide each entry of the column by this maximum value.\n",
    "\n",
    "$X_{\\text{scaled}} = \\frac{X_i - \\max(|X|)}{\\max(|X|)}$\n",
    "\n",
    "After performing the above-mentioned two steps we will observe that each entry of the column lies in the range of -1 to 1. But this method is not used that often the reason behind this is that it is too sensitive to the outliers. And while dealing with the real-world data presence of outliers is a very common thing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   LotArea  MSSubClass\n",
      "0     8450          60\n",
      "1     9600          20\n",
      "2    11250          60\n",
      "3     9550          70\n",
      "4    14260          60\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('SampleFile.csv')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's apply the first method which is of the absolute maximum scaling. For this first, we are supposed to evaluate the absolute maximum values of the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(215245)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "max_vals = np.max(np.abs(df))\n",
    "max_vals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are supposed to subtract these values from the data and then divide the results from the maximum values as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       LotArea  MSSubClass\n",
      "0    -0.960742   -0.999721\n",
      "1    -0.955400   -0.999907\n",
      "2    -0.947734   -0.999721\n",
      "3    -0.955632   -0.999675\n",
      "4    -0.933750   -0.999721\n",
      "...        ...         ...\n",
      "1455 -0.963219   -0.999721\n",
      "1456 -0.938791   -0.999907\n",
      "1457 -0.957992   -0.999675\n",
      "1458 -0.954856   -0.999907\n",
      "1459 -0.953834   -0.999907\n",
      "\n",
      "[1460 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print((df - max_vals) / max_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Min-Max Scaling\n",
    "\n",
    "This method of scaling requires below two-step:\n",
    "\n",
    "1. First we are supposed to find the minimum and the maximum value of the column.\n",
    "2. Then we will subtract the minimum value from the entry and divide the result by the difference between the maximum and the minimum value.\n",
    "\n",
    "$$\n",
    "X_{\\text{scaled}} = \\frac{X_i - X_{\\min}}{X_{\\max} - X_{\\min}}\n",
    "$$\n",
    "\n",
    "As we are using the maximum and the minimum value this method is also prone to outliers but the range in which the data will range after performing the above two steps is between 0 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LotArea</th>\n",
       "      <th>MSSubClass</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.033420</td>\n",
       "      <td>0.235294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.038795</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.046507</td>\n",
       "      <td>0.235294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.038561</td>\n",
       "      <td>0.294118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.060576</td>\n",
       "      <td>0.235294</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    LotArea  MSSubClass\n",
       "0  0.033420    0.235294\n",
       "1  0.038795    0.000000\n",
       "2  0.046507    0.235294\n",
       "3  0.038561    0.294118\n",
       "4  0.060576    0.235294"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(df)\n",
    "scaled_df = pd.DataFrame(scaled_data, \n",
    "                         columns=df.columns)\n",
    "scaled_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Normalization\n",
    "\n",
    "Normalization is the process of adjusting the values of data points so that they all have the same length or size, specifically a length of 1. This is done by dividing each data point by the \"length\" (called as Euclidean norm) of that data point. Think of it like adjusting the size of a vector so that it fits within a standard size of 1.\n",
    "\n",
    "The formula for Normalization looks like this:\n",
    "\n",
    "$$\n",
    "X_{\\text{scaled}} = \\frac{X_i}{\\|X\\|}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $X_i$ is the $i^{\\text{th}}$ element of the vector $X$\n",
    "- $\\|X\\|$ is the Euclidean norm: $\\|X\\| = \\sqrt{\\sum_{i=1}^n X_i^2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    LotArea  MSSubClass\n",
      "0  0.999975    0.007100\n",
      "1  0.999998    0.002083\n",
      "2  0.999986    0.005333\n",
      "3  0.999973    0.007330\n",
      "4  0.999991    0.004208\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "scaler = Normalizer()\n",
    "scaled_data = scaler.fit_transform(df)\n",
    "scaled_df = pd.DataFrame(scaled_data,\n",
    "                         columns=df.columns)\n",
    "print(scaled_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Standardization\n",
    "This method of scaling is basically based on the central tendencies and variance of the data. \n",
    "\n",
    "1. First we should calculate the mean and standard deviation of the data we would like to normalize it.\n",
    "2. Then we are supposed to subtract the mean value from each entry and then divide the result by the standard deviation.\n",
    "\n",
    "This helps us achieve a normal distribution of the data with a mean equal to zero and a standard deviation equal to 1.\n",
    "\n",
    "$$\n",
    "X_{\\text{scaled}} = \\frac{X_i - X_{\\text{mean}}}{\\sigma}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    LotArea  MSSubClass\n",
      "0 -0.207142    0.073375\n",
      "1 -0.091886   -0.872563\n",
      "2  0.073480    0.073375\n",
      "3 -0.096897    0.309859\n",
      "4  0.375148    0.073375\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(df)\n",
    "scaled_df = pd.DataFrame(scaled_data,\n",
    "                         columns=df.columns)\n",
    "print(scaled_df.head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Robust Scaling\n",
    "\n",
    "In this method of scaling, we use two main statistical measures of the data.\n",
    "\n",
    "- Median\n",
    "- Inter-Quartile Range\n",
    "\n",
    "After calculating these two values we are supposed to subtract the median from each entry and then divide the result by the interquartile range.\n",
    "\n",
    "$$\n",
    "X_{\\text{scaled}} = \\frac{X_i - X_{\\text{median}}}{\\text{IQR}}\n",
    "$$\n",
    "\n",
    "**Why use Robust Scaling?**\n",
    "Imagine a dataset of house prices where most houses are between $100k and $500k, but there's one mansion priced at $10 million.\n",
    "\n",
    "If you used Min-Max or StandardScaler, that mansion would skew the scaling, making the rest of the data compressed toward zero.\n",
    "\n",
    "RobustScaler, however, ignores those extreme values, focusing only on the middle 50% (the interquartile range), so your data ends up being more evenly scaled.\n",
    "\n",
    "**Example** Let’s say we have the following values for a feature:\n",
    "```\n",
    "[1, 2, 2, 3, 4, 5, 100]\n",
    "```\n",
    "- Median = 3\n",
    "- Q1 = 2, Q3 = 5 → IQR = 5 - 2 = 3\n",
    "\n",
    "Now apply Robust Scaling:\n",
    "```(1 - 3) / 3 = -0.67\n",
    "(2 - 3) / 3 = -0.33\n",
    "(2 - 3) / 3 = -0.33\n",
    "(3 - 3) / 3 =  0.0\n",
    "(4 - 3) / 3 =  0.33\n",
    "(5 - 3) / 3 =  0.67\n",
    "(100 - 3) / 3 = 32.33  ← still large, but doesn't affect the rest\n",
    "```\n",
    "The middle values are well-scaled, and the outlier (100) is still high but doesn't **squash** the rest of the data.\n",
    "\n",
    "**When to Use RobustScaler?**\n",
    "\n",
    "Use it when\n",
    "- Your data has outliers.\n",
    "- You want resilient and stable scaling for models sensitive to feature scales (like SVMs, k-NN, or logistic regression)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    LotArea  MSSubClass\n",
      "0 -0.254076         0.2\n",
      "1  0.030015        -0.6\n",
      "2  0.437624         0.2\n",
      "3  0.017663         0.4\n",
      "4  1.181201         0.2\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "scaler = RobustScaler()\n",
    "scaled_data = scaler.fit_transform(df)\n",
    "scaled_df = pd.DataFrame(scaled_data,\n",
    "                         columns=df.columns)\n",
    "print(scaled_df.head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scaler Comparison Table\n",
    "\n",
    "| **Scaler**                       | **Best When...**                                                          | **Sensitive to Outliers?** | **Typical Output Range** | **Use Case Examples**                                |\n",
    "| -------------------------------- | ------------------------------------------------------------------------- | -------------------------- | ------------------------ | ---------------------------------------------------- |\n",
    "| **1. Absolute Max Scaling**      | Data needs to be scaled relative to the largest absolute value            | Yes                      | -1 to 1                  | Simple range compression, rarely used in practice    |\n",
    "| **2. Min-Max Scaling**           | You want to bring data to a fixed range (e.g., 0–1) and no/few outliers   | Yes                      | 0 to 1 (or custom range) | Neural networks, image data, constrained algorithms  |\n",
    "| **3. Normalization (L1/L2)**     | You want to scale **samples** to unit norm (rows sum to 1 or L2 norm = 1) | Yes                      | Vector norm = 1          | Text classification (TF-IDF), KNN, cosine similarity |\n",
    "| **4. Standardization (Z-Score)** | Features are normally distributed or model assumes normal distribution    | Yes                      | Mean = 0, Std = 1        | Logistic regression, SVMs, PCA                       |\n",
    "| **5. Robust Scaling**            | Your data has outliers and you don’t want them to skew your model         | No                       | Centered around 0        | Regression, SVMs, tree-based models with outliers    |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
